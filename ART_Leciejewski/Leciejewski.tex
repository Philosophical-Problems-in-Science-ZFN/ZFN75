\begin{artengenv}{Sławomir Grzegorz Leciejewski}
	{New experimentalism and computer-aided experiments}
	{New experimentalism and computer-aided experiments}
	{New experimentalism and computer-aided experiments}
	{Adam Mickiewicz University in Poznań}
	{In the 1980s, computer-aided experimental research became standard in the majority of good research laboratories. Unfortunately, back then this was not properly reflected in the professional literature related to the philosophy and methodology of science. As a~matter of fact, a~new experimentalism did emerge, and this sort of philosophy of experiment, according to its creators, was proposed in order to adequately describe the experimental practice (this will be later discussed in the first part of this article), however, in the initial phase of its development, it omitted in its analyses the role of computers in experimental research (see the second part of this article). This seems to be the greatest oversight of the philosophers of science being the creators of the new experimentalism (see the third part of this article) and calls for supplementation (see the fourth part of this article). It is true that the turn of the 20th and 21st century saw a~number of philosophical analyses related to computer experiments. These include, e.g., computer simulations, however I~am only interested in classic experiments whose performance is enabled by various computer systems (e.g. LHC at CERN). In the final part of this article I~will present examples of aspects of experimental works that have not yet been analyzed and that may, in fact, supplement the new experimentalism with the analyses of computer-aided experiments.
	}
	{philosophy of science, computer-aided experiment, new experimentalism.}


\newcounter{saveenum}



\section*{Introduction}

\lettrine[loversize=0.13,lines=2,lraise=-0.03,nindent=0em,findent=0.2pt]%
{T}{}he development of computers, software and peripheral devices has enabled a~more efficient use of computing, testing, advisory, diagnostic, monitoring, measuring and controlling functions, as well as a~number of others; it has triggered the use of computers in virtually any area of human activity. Computer sciences as such, being a~group of theoretical (mathematical methods, logic, theory of automates, theory of algorithms, mathematical linguistics), technical (the structure of computer equipment and development of software) as well as application branches of science (application of computer sciences in various fields) have currently been developing extremely fast. One of the crucial uses of computers is supporting scientific research in empirical sciences.

\enlargethispage{1.5\baselineskip}

In the 1980s, computer-aided experimental research became standard in the majority of good research laboratories 
%\label{ref:RNDBF5RrjHXHj}(Crowley-Milling, 1974).
\parencite[][]{crowley-milling_computer_1974}. %
 Unfortunately, back then this was not properly reflected in the professional literature related to the philosophy and methodology of science.



As a~matter of fact, a~new experimentalism did emerge, and this sort of philosophy of experiment, according to its creators, was proposed in order to adequately describe the experimental practice (this will be later discussed in the first part of this article), however, in the initial phase of its development, it omitted in its analyses the role of computers in experimental research (see the second part of this article). This seems to be the greatest oversight of the philosophers of science being the creators of the new experimentalism (see the third part of this article) and calls for supplementation (see the fourth part of this article).



It is true that the turn of the 20th and 21st century saw a~number of philosophical analyses related to computer experiments. These include, e.g., computer simulations 
%\label{ref:RNDqFQ8OVOy2Z}(Bartz-Beielstein, 2005; Giere, 2009; Guala, 2002; Hughes, 1999; Humphreys, 1995; Morgan, 2003; Peschard, 2009; Winsberg, 2010; Burge, 1998; Epstein, 1999; Hartmann, 1996; Lenhard, 2007; Morrison, 2009; Parker, 2013),
\parencites[][]{bartz-beielstein_new_2005}[][]{giere_is_2009}[][]{guala_models_2002}[][]{morgan_ising_1999}[][]{humphreys_computational_1995}[][]{morgan_experiments_2003}[][]{peschard_modeling_2009}[][]{winsberg_science_2010}[][]{burge_computer_1998}[][]{epstein_agent-based_1999}[][]{hartmann_world_1996}[][]{lenhard_computer_2007}[][]{morrison_models_2009}[][]{parker_computer_2013}, %
 however I~am only interested in classic experiments whose performance is enabled by various computer systems (e.g. LHC at CERN). In the final part of this article I~will present examples of aspects of experimental works that have not yet been analyzed and that may, in fact, supplement the new experimentalism with the analyses of computer-aided experiments.

\enlargethispage{2\baselineskip}

\section*{New experimentalism}

It is obvious to many philosophers of science that theory is the basic structural unit of knowledge within the empirical disciplines. The supporters of such an approach to theoreticism also analyze the experimental practice arguing, however, that theories themselves should, in fact, determine the possibility of conducting experiments, the principles of the construction of research equipment and the ways of interpreting the results obtained in the course of experimental research. However, theoreticism, when juxtaposed with actual research practice, appears to be a~grossly inadequate description of that practice. This prompted Ian Hacking to propose a~new program for philosophical reflection on science, which was later known as ``new experimentalism'' 
%\label{ref:RNDRdtiZrc1g6}(Hacking, 1983; Ackermann, 1989).
\parencites[][]{hacking_representing_1983}[][]{ackermann_new_1989}.%




New experimentalism was created by philosophers (Ian Hacking, Peter Galison, Allan Franklin) who were convinced that the philosophical reflection on empirical sciences should be conducted starting from real experimental practice and considering theoretical scientific practice in its context. The representatives of the new experimentalism follow the achievements of science, write down contemporary experimental stories related mainly to high energy physics, assist in the course of experiments, represent a~high level of knowledge of physics and the principles of construction of research equipment.



Hacking's philosophy of science can be seen as belonging to the study area of problem-solving activity, yet it is fundamentally different from other concepts of this type (e.g. those of Thomas Kuhn or Larry Laudan). Solving research problems is not, according to Hacking, solving the puzzles of normal science within a~particular paradigm, nor is it a~measure of the theoretical progress of science. Most of the research problems present in the natural sciences are empirical problems arising in the course of experimental research practice 
%\label{ref:RND6Ra99J7CVq}(Schummer, 2021).
\parencite[][]{sobczynska_why_2021}.%

\enlargethispage{1.5\baselineskip}


Hacking also weakens the thesis of the complete theoretical dependence of the experiment. He does not claim that experimentation can take place without making any assumptions, yet he believes that in many cases theories were created on the basis of pre-theoretical experiments 
%\label{ref:RNDSclUNXniGz}(Hacking, 1983).
\parencite[][]{hacking_representing_1983}.%




Hacking also claims that the analysis of the research practice of empirical sciences suggests that it is dominated by experimental practice and that theorizing is not a~homogeneous form of scientific work but it is broken down into a~series of activities such as: speculation, calculation and building models 
%\label{ref:RNDLNdzmlu3Q2}(Hacking, 1983, pp.210–217).
\parencite[][pp.210–217]{hacking_representing_1983}. %
 According to this philosopher of science, theoretical research and experimental discoveries often proceed independently and only later are they combined to create theoretically developed scientific facts (e.g. the discovery of positrons or relic radiation). Thus, according to Hacking, the role of scientific experiments is not merely limited to situations in which a~choice is made between competing theories or to procedures for testing scientific theories.



A~crucial postulate of the new experimentalism is also assigning a~fundamental role in scientific research to tampering with, acting and intervening in the world. The activity of scientists, therefore, consists essentially in conscious intervening in the world, and, to a~much lesser extent, in representing it in scientific theories 
%\label{ref:RNDr7BkUQq2Vv}(Hacking, 1983, pp.153–154).
\parencite[][pp.153–154]{hacking_representing_1983}. %
 Thus, science cannot be reduced only to learning about and representing the world. Science is also acting and intervening in the world. The new experimentalists therefore propose a~new vision of science, in which science becomes not so much knowledge as practice. The culture of science is therefore not limited to theories (as in the tradition of logical empiricism) or paradigms (as proposed by Kuhn), but consists of many different elements that enter into relationships with each other.

\enlargethispage{1.5\baselineskip}

As already indicated, according to new experimentalists, one of the important roles of the experiment is the creation of new phenomena that fail to occur in nature in a~pure state. In the late 19\textsuperscript{th} century physicists began to call these phenomena ``effects'' (Compton effect, photoelectric effect, piezoelectric effect, etc.). According to Hacking, ``to experiment is to create, produce, refine and stabilize phenomena'' 
%\label{ref:RND2W3nCkrSl2}(Hacking, 1983, p.230).
\parencite[][p.230]{hacking_representing_1983}.%




New experimentalists also believe that experimental activity in science is now becoming a~largely autonomous field. The own life of the experiment manifests itself in various areas. One of them is the dichotomy of the aforementioned ``theoretical cultures'' and ``experimental cultures'' that became increasingly clear in the 20\textsuperscript{th} century. Another area is the close connection between experimental work and technique and technology. The third area is the sometimes significant non-theoretical or a-theoretical nature of experimental practice (e.g. PEGGY II) 
%\label{ref:RNDvleeiceDQO}(Hacking, 1984, pp.161–170).
\parencite[][pp.161–170]{leplin_experimentation_1984}.%


%\enlargethispage{1.5\baselineskip}

Hacking 
%\label{ref:RNDXztLOM6qId}(1985)
\parencite*[][]{churchland_we_1985} %
 and Franklin 
%\label{ref:RNDIOT6L1nGKk}(1986, pp.226–243)
\parencite*[][pp.226–243]{franklin_neglect_1986} %
 also analyze the issue of ``fraud'' produced by research equipment on the example of microscopic artifacts as each experimental device produces its own effects, generally known as ``noise''. These effects arise as a~result of the work of the apparatus itself without the contribution of the tested object. It is natural that the undesirable effects of the work of experimental apparatus raise anxiety among naturalists and philosophers of science. However, according to the new experimentalists, it is unnecessary to exaggerate the negative significance of artifacts. In the functional-engineering approach to the research apparatus, it is possible to find ways of exposing the aforementioned undesirable effects. With regard to microscopes, Hacking presents three basic ways of distinguishing artifacts from real images: on the basis of the grid\footnote{Scaled grids are prepared for microscopic observation of various objects. The drawing of the grid made by the researcher is subject to the process of photographic reduction, and then enlarged under the microscope as many times as it was reduced. The person using the microscope receives an image of a~grid with the same square size as the original one. The researcher's control over the work of the apparatus\textrm{---}from preparing the grid to observing the magnified image---convinces them that they are observing a~real image, not an artifact 
%\label{ref:RNDO13ild509z}(Hacking, 1985).
\parencite[][]{churchland_we_1985}. %
 }, coincidence\footnote{Apart from optical microscopes, we currently also use electron, fluorescence, polarizing, acoustic ones, and others. If the image of a~given specimen seen through each of these instruments looks the same, it is a~confirmation of the reliability of the images from different microscopes. Different types of microscopes operate under completely different physical laws and it would be strange if different theories about the functioning of different types of microscopes were false in such a~way that each camera would produce exactly the same artifact 
%\label{ref:RNDFsHDCD4XJC}(Hacking, 1985).
\parencite[][]{churchland_we_1985}.%
} and the ``blind test''\footnote{The blind test method (calibration) consists in both the suspension and the slide in the suspension being examined separately to check if the suspension does not give an absorption signal in the expected specimen wavelength range (e.g. in IR spectroscopy). The spectrum of the substance is taken into account only when the result of the blind test is negative 
%\label{ref:RND6CEHEUNwwc}(Franklin, 1986).
\parencite[][]{franklin_neglect_1986}.%
} method 
%\label{ref:RNDavt7IspJ0U}(Hacking, 1985, pp.145–151).
\parencite[][pp.145–151]{churchland_we_1985}.%




I~will return to the methods of unmasking artifacts in the context of computer-aided experimental research systems in the last section. I~will then compare the main theses of new experimentalism with contemporary computer-aided experimental practice. This will be used to support the thesis that it is necessary to further develop new experimentalism so that it constitutes a~philosophy of experiment that would be adequate also in the 21\textsuperscript{st} century.



\section*{Computer-aided experimental research }

One of the crucial applications of computers is to support research in empirical sciences. Contemporary computer functions in empirical sciences can be divided into three main groups: analytical (on-line), synthetic (off-line) and presentational (on-line and off-line) 
%\label{ref:RNDgIRHS0BX1l}(Leciejewski, 2019; 2018).
\parencites[][]{leciejewski_preface_2019}[][]{leciejewski_struktura_2018}. %
 The first group involves cases when the computer is directly connected to the measuring instrument (consisting of a~measuring device, analog-to-digital converters and interface) and is mainly used for the collection and preliminary analysis of empirical data coming from the experimental set. This group of computer applications in empirical sciences includes:



\begin{enumerate}

\item retrieving empirical data from measuring devices using analog-to-digital converters (A/D) and interfaces as well as controlling the course of the experiment through digital-to-analog converters (D/A) and actuators (this computer function will be subject to a~detailed discussion later in this article);

\item gathering empirical data (creating digital empirical databases);

\item comparing empirical data with theoretical data.

\end{enumerate}

In the second group of applications, the computer is no longer directly connected to the experimental set but is mainly used to process the previously gathered empirical data. This group of computer functions includes:



\setcounter{saveenum}{\value{enumi}}

\begin{enumerate}

\setcounter{enumi}{\value{saveenum}}

\item formulating simple phenomenological laws (computer inductive generalizations formulated on the basis of digital empirical databases);

\item numerical justification of further experiments (optimization of further experiments by narrowing down the possible class of experiments);

\item computer simulations of the course of phenomena/processes (based on gathered empirical data and assumed theories);

\item design and optimization of new, computer-aided experimental sets.

\end{enumerate}

An important class of computer applications is the presentation of the processed empirical data (from the first group---points 1-3) and of the obtained results of numerical analyzes (from the second group---points 4-7). Visualization can take place during the operation of the computer as part of the experimental set (on-line mode) and outside of it (off-line mode). This group of computer applications in empirical sciences includes:

\enlargethispage{1.5\baselineskip}

\setcounter{saveenum}{\value{enumi}}

\begin{enumerate}

\setcounter{enumi}{\value{saveenum}}

\item visualization of the empirical data and obtained results of numerical analyses,

\item electronic communication between research centers (the exchange of data, simulations and visualizations),

\item optimization of the human-machine communication processes (scientist---computer system supporting scientific research)\footnote{It is quite obvious that this is not a~disjoint division. Some points overlap when it comes to their scope, e.g. 8 is partly contained in 6, 5 intersects with 6, similarly as 6 and 7 (however the latter ones to a~small extent).}.

\end{enumerate}

In general, however, there are three interacting factors in experimental research:



\begin{enumerate}[label=\Alph*.]

\item the experimenter, i.e. the subject stimulating the experiment and interpreting its results;

\item the tested object, i.e. the object of the experimental research;

\item and what mediates between them, i.e. the experimental research automation system (nowadays, it is usually a~computer-aided experimental research system\footnote{A~computer-aided experimental research system is a~set of methods and means used in order to improve, in compliance with the general assumptions of the (scientific, technical, medical, etc.) experiment, the processes of collecting information on the tested object and its processing by means of computer technology. }).

\end{enumerate}

In contemporary computer-aided experimental set, several hardware elements can be distinguished, constituting one functional whole being the first of the above-mentioned computer functions in empirical sciences. In the system in question, the information from the object of the experimental research is gathered using measuring devices (sensors\footnote{The sensor converts the measured quantity (e.g. temperature) to another physical quantity (e.g. DC voltage), which is easier to measure or more convenient to transmit over a~distance (the input quantity of the sensor is the measured quantity).}). Subsequently, this analog information is pre-processed using analog-to-digital converters\footnote{Thanks to the analog-digital converter, the information from measuring devices (sensors) can be obtained in the form of data that will be digitally processed using a~computer with software. Converting an analog quantity into a~digital signal consists of three operations: sampling (signal discretization in time), quantization (signal value discretization) and coding.}. The digitized data is then transferred via various interfaces\footnote{The interface is a~type of digital-to-digital converter that can be either a~series or parallel.} to a~computer\footnote{The computer being part of the experimental set can perform various functions in this system: control the course of the experiment (through the interface, digital-to-analog converters and actuators), record and process data coming from the measuring device (through the analog-to-digital converter and interface), operate the peripherals (monitor, keyboard, mouse, printer) used for controlling the experimental set and presenting the measurement and calculation results, control data transmission outside the experimental set (e.g. via the Internet). }. There, the information---as a~result of the operation of various kinds of software\footnote{The most popular programming environment used to support experimental work is LabVIEW using the graphical programming language G.}---can be processed, stored and made available (for example in the form of a~visualization). A~computer with appropriate software can also control the course of the experiment through interfaces, digital-to-analog converters and actuators.



From the perspective of computerization of contemporary experiments, it is worth considering whether the use of computer-aided experimental research introduces only indisputable quantitative changes to experimental work, or if we are also dealing here with qualitative changes. Does the ``distance'' between the subject (A) and the object of the experiment (B) change due to the use of analog-to-digital converters and interfaces (C)? Is the interpretation of the results of experiments with the experimental research supported by a~computer different from the interpretation of the results of classic empirical research? Does the use of numerical methods introduce a~different type of justification of scientific hypotheses---namely a~numerical justification? Does the status of the experimenter in empirical sciences change in a~qualitative way when the scientific research is supported by computers?



In the initial phase of the development of the new experimentalism such questions, crucial from the perspective of the philosophy of experimental sciences, were not even posed by its representatives, and thus no answers were given to them. In the following paragraph, I~will also present other shortcomings of this philosophy of experiment in comparison with contemporary computerized research practice and, subsequently, determine research fields which, once developed, would enable the emergence of a~new version of the new experimentalism. It turns out that the instruments used in computer-aided experimental research imply the need to reformulate a~number of theses advanced by the supporters of the existing version of the new experimentalism.



\section*{New experimentalism and computer-aided experimental research: problems}

Undoubtedly, the representatives of the new experimentalism have significantly appreciated the role of experiment in scientific research. Together, they opposed the dismissive treatment of the realities of experimental practice in the analyses of the philosophy and history of science. An important contribution of the new experimentalism to the philosophy of science is the analysis of the new role that an experiment can play. It is the creation of new phenomena that do not or cannot occur in nature in a~pure state. According to the representatives of the new experimentalism, experimenting does not only mean testing theories but above all---creating, producing, refining and stabilizing phenomena.



Hacking noted that the so-called laboratory science emerged already in the 17\textsuperscript{th} century. It is characterized by the construction of apparatus intended to isolate and purify the existing phenomena and to create new ones 
%\label{ref:RNDazQgiMctcp}(Hacking, 1996).
\parencite[][]{hacking_disunities_1996}. %
 Today, this type of equipment is aided by computer systems. Hacking himself also notes that one of the unifying factors that bring together sciences are certain tools which include fast computer calculations (it is quite surprising that he does not include computers among the tools, but, instead, fast computer calculations 
%\label{ref:RNDfrwZZzSn19}(Hacking, 1996)
\parencite[][]{hacking_disunities_1996}%
). Unfortunately, his analysis of this issue cannot be exhaustive, as it only spreads over a~single paragraph of the cited article. Hacking claims in it that thanks to fast numerical calculations, we can formulate new theories and process large amounts of empirical data. Examples of this type of computer calculations, according to him, are the counts of data coming from a~telescope with many small mirrors as well as virtual acoustic designs of theater architecture 
%\label{ref:RNDGZkCnVTa0f}(Hacking, 1996).
\parencite[][]{hacking_disunities_1996}.%




The above remarks made by Hacking indicate that he does not take into account the specificity of computer-aided experiments, as---firstly---he reduces the role of computers in empirical research only to fast computer calculations (in the previous paragraph I~listed nine other functions that computers can perform in empirical sciences). Secondly, he claims that, thanks to these calculations, it is possible to formulate new theories, which currently is not feasible 
%\label{ref:RNDPsAZZYGpFO}(Leciejewski, 2013, pp.86–93)
\parencite[][pp.86–93]{leciejewski_cyfrowa_2013}%
\footnote{The main objective of this book is to provide answers to two fundamental questions from the field of philosophical reflection on science and its development. Firstly, if the use of computer in empirical studies has created a~brand new computer style of scientific research; secondly, whether computer has revolutionized experimental studies. When providing the answers, the monograph refers to the well-known concepts of thought developed by Ludwik Fleck, the style of scientific research by Alistair Cameron Crombie and its further modifications, as much as to several concepts of scientific revolutions (by Thomas Samuel Kuhn, Bernard Cohen and Steven Shapin). These ideas in the nutshell could be found also in 
%\label{ref:RNDxPguYKtGME}(Leciejewski, 2018).
\parencite[][]{leciejewski_struktura_2018}.%
}.



The new experimentalists argue that many scientific experiments are non-theoretical or a-theoretical. This thesis is valid for chemistry, however, in physics fundamental theories play a~much greater role than, for example, in chemistry 
%\label{ref:RNDIHl06pqhSq}(Zeidler and Sobczyńska, 1995).
\parencite[][]{zeidler_idea_1995}. %
 In modern physics, laboratory research is aimed at confirming a~general theory. For example, CERN's largest physics laboratory and most complex and intricate research facility, the Large Hadron Collider, was built mainly to test a~certain theoretical concept explaining the origin of hadron masses. This experiment was conducted with a~view to confirming the existence of the so-called Higgs field by finding a~particle mediating interactions with this field, i.e. the so-called Higgs boson 
%\label{ref:RND1PRQ2Lu5J8}(Bhat, 2013).
\parencite[][]{bhat_observation_2013}. %
 The idea of such a~new particle appeared in an article by Peter Higgs published in 1964 in which the author proposed a~theoretical explanation for the origin of the mass of elementary particles 
%\label{ref:RNDEBQf9cn0tz}(Higgs, 1964).
\parencite[][]{higgs_broken_1964}.%




It is worth noting that the Higgs mechanism played a~key part in the development of the theory of the electroweak interaction by Steven Weinberg 
%\label{ref:RNDoam4nb2t1P}(1967).
\parencite*[][]{weinberg_model_1967}. %
 Without this mechanism, the unification of the electromagnetic and nuclear weak interactions would be impossible. The theory of electroweak interactions resulted in many predictions that could be verified experimentally. These were, for example, two new types of particles, W~and Z~bosons, responsible for the transfer of weak interactions. They were discovered in 1983, in the SPS (Super Proton Synchrotron) accelerator operating at CERN since 1976. One of the main research objectives of this accelerator was to indirectly confirm the electroweak theory by discovering new particles 
%\label{ref:RNDyygCZCrBlc}(Weinberg, 1992).
\parencite[][]{weinberg_dreams_1992}. %
 This experiment was therefore aimed at confirming the general theory.



It needs to be emphasized that already at the time of the emergence of the new experimentalism (in the 1980s), computers played a~crucial part in the experimental research. The creators of this philosophy of experiment, however, fail to observe this fact, and---what is worth pointing out---the role of computers in the experimental research was already significant at that time. To support this thesis, I~will present two examples of the use of computers in research work, which were either known to the creators of the new experimentalism (as they write about them themselves), or commonly known when the new experimentalism was emerging (the existence of the CERN laboratory).



Computers have been widely used at CERN since the early 1970s 
%\label{ref:RNDOvIx85S4HY}(Crowley-Milling, 1974).
\parencite[][]{crowley-milling_computer_1974}. %
 Their role in the above-mentioned discovery of theoretically predicted bosons mediating weak interactions (the Super Proton Synchrotron accelerator which was transformed into a~proton-antiproton collider) in 1983 was crucial. Without computers, the entire device was unable to function. It is hard to believe that Hacking did not hear about the most computerized laboratory in the world (i.e. CERN) and did not know about the role of computers in the experiments carried out there for already over a~decade, especially since he himself gave numerous examples related to high energy physics, thus he for sure must have been familiar with the most important laboratory dealing with this particular branch of physics.



In addition, in the PEGGY II experiment described by Hacking, it was in fact the computer that was responsible for recording the polarization direction for each pulse (as reported by Hacking himself 
%\label{ref:RND7emScn2s9U}(Hacking, 1984, p.164)
\parencite[][p.164]{leplin_experimentation_1984}%
), thus---and it is worth emphasizing---without the computer the entire device would be worthless. However, this aspect of the functionality of the PEGGY II device is altogether disregarded by the mentioned philosopher and is not subject to a~methodological analysis. Yet already in 1978 (the creation of PEGGY II 
%\label{ref:RND4e7n0Xh2Y5}(Hacking, 1984, p.162)
\parencite[][p.162]{leplin_experimentation_1984}%
) an important part of the experimental apparatus analyzed (in 1984) by Hacking was the computer, although the author ignores this fact. Thus, based on the analysis of the works of representatives of the new experimentalism, it can be concluded that they failed to fully comprehend the significance of computers in experimental research.



It should therefore be concluded that the failure to take into account the role of the computer together with the appropriate software (and analog-to-digital converters) in experimental research is a~serious oversight of the representatives of the new experimentalism. Hacking postulates that the philosophy of science should begin with the analysis of actual research practice, and not only focus on the analysis of its products. Unfortunately, he fails to observe the fact that the actual research practice of the last twenty years of the 20\textsuperscript{th} century and the beginning of the 21\textsuperscript{st} century was indeed dominated by computer-aided experimental research systems. Due to this significant omission, the new experimentalism it its initial phase was not a~methodological concept that would adequately reconstruct contemporary experimental practice, as it is largely computer-aided. In support of this thesis, I~will give some examples of results obtained by representatives of the new experimentalism which cannot be easily applied to modern computer-aided experiments carried out using even such simple experimental sets as those described in the previous paragraph. It will at least partially justify the need to supplement the new experimentalism.



Hacking and Franklin investigate the emergence of artifacts in research equipment. As we know, each experimental device generates noise resulting from the operation of the experimental apparatus without the tested object. According to the new experimentalists, there is no need to exaggerate the negative impact of artifacts, as there are ways to expose such undesirable effects. The entire analysis of this issue by Hacking is based on one example only---various types of microscopes. However, as this is not the only research tool, it is worth checking whether the methods of exposing artifacts postulated by this philosopher can also be applied to commonly used computer-aided experimental sets.



For example, Hacking's argument from coincidence applies to microscopic techniques, thus it is not universal. Nowadays, in most empirical sciences, we perceive objects not only with the help of a~microscope, but mainly with the help of computer systems. Therefore, one should try to reformulate the argument from coincidence in such a~way that it would also refer to contemporary scientific work, i.e. perceiving with the use of a~computer 
%\label{ref:RNDj9dEoj369D}(Bialynicki-Birula and Bialynicka-Birula, 2004).
\parencite[][]{bialynicki-birula_modeling_2004}.%




From the perspective of computer-aided experimental sets, one should look for coincidences between the empirical research conducted without the use of a~computer and that in which the computer is a~part of the experimental set. This would refer to the process of obtaining empirical data, i.e. to the first two computer functions in the empirical sciences (listed in the previous paragraph). The second coincidence would have to refer to the analysis and processing of the obtained empirical data, i.e. to the remaining tasks of the computer (listed in the previous paragraph). If a~given experiment could be conducted analogically and the data processed analytically, and the same results were to be obtained as in the case of a~computer-aided experiment with a~numerical analysis of empirical data, it would undoubtedly strengthen the importance of the results obtained. Therefore, we would have two more arguments from coincidence: analog-digital and analytical-numerical. However, I~am afraid that in the vast majority of cases conducting such comparative research is not possible. It is difficult to imagine contemporary non-computerized research conducted in the field of elementary particle physics, e.g. analogous to those conducted at CERN, which collects 30 PB of digital data 
%\label{ref:RNDyB1QQtC5Kx}(Leciejewski, 2015)
\parencite[][]{leciejewski_digital_2015} %
 or, for example, analytical calculations of the dynamics of the observable Universe involving only the determination of the trajectory of 150 billion galaxies. The mere analytical justification of the stability of the Solar System is not possible, let alone modeling the dynamics of the entire Universe.



It is, therefore, evident that the theoretically possible arguments from analog-digital and analytical-numerical coincidence are unfortunately inapplicable in practice. Therefore, the problem of exposing artifacts in digitally-aided experimental sets can be solved neither using the methods proposed by Hacking (grid-based, coincidence-based, blind test method) nor applying their modifications proposed above. The problem of the negative significance of artifacts in modern science cannot be, therefore, ignored, as the representatives of the new experimentalism would like, claiming that there are reliable methods of exposing them.



\section*{New experimentalism and computer-aided experimental research: perspectives}

In the following part of this article I~will analyze, as I~did so far, only the computer-aided experiments. I~will skip in my study computer experiments, i.e. various types of computer simulations. They might be considered a~next step in the development of the new experimentalism, if one could prove that they differ fundamentally from real experiments performed on physical objects. In light of the related long-standing discussion, it is hard to equate real experiments of that kind with computer simulations\footnote{There is a~large body of literature in the philosophy of science that includes attempts to determine whether computer simulations are classic experiments, a~type of theoretical work or some new hybrid method of doing science.
Eric Winsberg
%\label{ref:RNDsTCp7JYcgx}(2010, p.136)
\parencite*[][p.136]{winsberg_science_2010}
 notes that ``We have […], rejected the overly conservative intuition that computer simulation is nothing but boring and straightforward theory application. But we have avoided embracing the opposite, overly grandiose intuition that simulation is a~radically new kind of knowledge production, ‘on a~par' with experimentation. In fact, we have seen that soberly locating simulation ‘on the methodological map' is not a~simple matter''. In the following part of my study I~will skip the seemingly unresolved and multi-faceted discussion regarding the relationship between computer simulations and classic experiments
%\label{ref:RNDiZAL7GXODC}(Kaufmann and Smarr, 1993; Humphreys, 1995; Hughes, 1999; Norton and Suppe, 2001; Guala, 2002; 2008; Morgan, 2003; Gilbert and Troitzsch, 2005; Giere, 2009; Morrison, 2009; Parker, 2009; 2017; Peschard, 2009; Winsberg, 2009; Parke, 2014)
\parencites[][]{kaufmann_supercomputing_1993}[][]{humphreys_computational_1995}[][]{morgan_ising_1999}[][]{miller_why_2001}[][]{guala_models_2002}[][]{guala_paradigmatic_2008}[][]{morgan_experiments_2003}[][]{gilbert_simulation_2005}[][]{giere_is_2009}[][]{morrison_models_2009}[][]{parker_does_2009}[][]{parker_computer_2017}[][]{peschard_modeling_2009}[][]{winsberg_tale_2009}[][]{parke_experiments_2014}%
. Thus, I~will not be interested in computer experiments (e.g. computer simulations of climate change, where \textit{x} amount of carbon dioxide is added to the atmosphere) but merely in the classic computer-aided experiments (e.g. those in which protons are accelerated to high speeds and made to collide with each other). The philosophical consequences of computer experiments have been broadly discussed, contrary to the philosophical consequences of computer-aided experiments.}.



Moreover, the new experimentalists have repeatedly spoken about intervening in the world 
%\label{ref:RNDtHl6Nn9Izl}(Hacking, 1983, pp.149–219)
\parencite[][pp.149–219]{hacking_representing_1983} %
 and the manipulative criterion of existence 
%\label{ref:RNDVTam9e7OWC}(Hacking, 1983, pp.220–232)
\parencite[][pp.220–232]{hacking_representing_1983} %
 and, in the case of computer simulations, this intervention and manipulation would be limited to electric currents in silicon devices and yet---so it seems---this is not necessarily the kind of ``experimentation'' the new experimentalists had in mind. In their works they analyzed real experiments, e.g. Hacking analyzed the Michelson-Morley experiment 
%\label{ref:RNDbRTPLBIyhG}(Hacking, 1983, pp.253–261),
\parencite[][pp.253–261]{hacking_representing_1983}, %
 Franklin---the measurement of the K\textsuperscript{+} experiment 
%\label{ref:RND53auqFBMVc}(Franklin, 1990, pp.115–131),
\parencite[][pp.115–131]{franklin_experiment_1990}, %
 while Galison---the early stages of seeking the intermediate vector bosons in weak W~and Z~interactions at CERN 
%\label{ref:RND3DdSAaiX6W}(Galison, 1987, pp.198–208).
\parencite[][pp.198–208]{galison_how_1987}. %
 Only Galison discussed in his publications issues related to the digital support used in experiments 
%\label{ref:RND1f1BKDGond}(Galison, 1997, pp.752–780).
\parencite[][pp.752–780]{galison_image_1997}. %
 However, also in this case these analyses still referred to real experiments and not to research being computer simulations exclusively 
%\label{ref:RNDwJXccciQ7P}(Galison, 1997, pp.689–752).
\parencite[][pp.689–752]{galison_image_1997}.%




Galison's analyzes mainly related to the analyzes of digital calculations carried out on the basis of previously obtained experimental data 
%\label{ref:RNDyYM9mk41YL}(Galison, 1997, pp.1–7, 752–771).
\parencite[][pp.1–7, 752–771]{galison_image_1997}. %
 Thus, it appears that several important aspects of computer-aided experimentation have escaped the attention of new experimentalists. These include: epistemological problems related to analog-to-digital processing in experimental systems and problems relating to the impossibility of archiving all empirical data generated by modern digitally supported experiments.



It is worth remembering that as a~result of natural phenomena, electrical signals corresponding to physical quantities such as: temperature, pressure, stress, radiation intensity, magnetic field strength, electrochemical potential, etc. are generated in measuring devices. These analog signals cannot be transmitted directly to the computer and require processing in analog-to-digital converters. This digital signal is transmitted to the computer via an interface. Also via interfaces (and digital-to-analog converters), the computer controls actuating devices (e.g. heaters, dosing valves, motors, radiation intensity regulators, etc.), which ensure control of the experiment parameters.



Most measuring devices respond to physical influences such as pressure, temperature, electrical voltage, liquid flow rate, etc., which change continuously within a~certain range. These are analog signals that must be converted to digital signals before they can be processed by computers. This change is made possible by analog-to-digital converters located at the meeting point of the analog and digital parts of the experimental system (between the measuring device and the interface plus the computer). Similarly, if digital signals from a~computer are to be used to control an experiment through analog actuators, they must be converted to an analog form using a~digital-to-analog converter\footnote{A~detailed description of how analog-to-digital converters work can be found in 
%\label{ref:RND8Yh3lNbX21}(Pelgrom, 2022).
\parencite[][]{pelgrom_analog--digital_2022}.%
}.



Crucial parameters of analog-to-digital converters include: resolution (the smallest size of the input signal distinguishable by the converter), frequency (the maximum number of input signal processing per unit of time) and processing time (the time elapsed between the input signal and the appearance of the encoded value at the output). These parameters determine the accuracy and speed of processing. It can therefore be said that each converter has a~specific ``inertia'' (processing time), which causes delays between the moment of occurrence of the examined phenomenon and the possibility of recording and processing the digital signal in a~computer system. Therefore, if the experimental system consists of many different measuring devices and many different analog-to-digital converters, there is a~problem of time synchronization of the data flowing to the computer. Each A/D converter may have different processing times and this must be taken into account when planning the experiment. This will result in a~slowdown in the operation of the experimental system---in accordance with the longest processing time of one of the A/D converters. All other converters will have to ``wait'' for the slowest one before the next cycle of time-synchronized measurements from all detectors begins.



The processing time of analog-to-digital converters only slows down the experimental system, yet the ``granularity'' of the converters (processing frequency) brings forth much more severe consequences. A~computer-aided experimental system may not ``notice'' rapidly changing processes taking place between the quantized moments of reading data from the measuring device. It is only possible to choose an appropriately fast converter if one knows how fast the changes in a~given parameter will be in the phenomenon under study, yet this is exactly what is to be determined in the very experiment! Therefore, it is impossible to properly design a~computer-aided experimental system without a~considerable knowledge about the tested object. Thus, it is difficult to talk about computer-aided atheoretical experiments.



The sampling frequency is also of great importance for the reliability and accuracy of the data that is transmitted between the measuring device and the computer. Without the knowledge of the phenomenon under study and the type of input data that will reach the analog-to-digital converter, it is impossible to select an appropriately accurate converter that meets the Kotelnikov-Shannon theorem (the sampling frequency cannot be less than twice the value of the highest frequency occurring in the signal) or the Nyquist theorem (a continuous signal can be recreated from a~discrete signal if it has been sampled at a~frequency at least twice the cut-off frequency of its spectrum). This further strengthens the thesis that it is impossible to conduct atheoretical computer-aided research. The very use of analog-to-digital converters in modern experimental research means that we must have some preliminary knowledge about the input signals of such converters. This, in turn, forces us to refer to theoretical knowledge regarding the phenomenon under study in order to be able to select the appropriate measuring device and analog-to-digital converter.



Similar conclusions can be drawn when analyzing the resolution parameter of the analog-to-digital converter. The input signal may change in such a~small range that the converter will not be able to distinguish these changes. If we do not know the changes that may potentially occur, we will not be able to select a~converter with the appropriate resolution.



Moreover, it is known that analog-to-digital converters generate numerous errors in the course of signal processing. The converter characteristics may not be linear, gain errors and zero offset errors may occur. Although the latter two can be eliminated by making an appropriate adjustment, there is no method to reduce linearity errors. Other errors (nonlinearity errors, total nonlinearity, total processing error, differential nonlinearity, differential nonlinearity coefficient, zero and scale thermal coefficients, differential nonlinearity thermal coefficient) often overlap and separating them is often impossible, as compensation for one error may cause an increase in another. This means that we will always be dealing with some processing error that we will not be able to eliminate and about which we will often know little. This results in the appearance of various types of artifacts in analog-to-digital converters. Moreover, there are no simple methods for exposing artifacts appearing in A/D converters, which are a~very important element emerging at the meeting point of the analog and digital parts of modern experimental systems.

\enlargethispage{1.5\baselineskip}

In addition to artifacts, another consequence of incorporating analog-to-digital converters into the experimental set is the emergence of a~qualitative principle that can be considered an analogy to the Heisenberg's uncertainty principle for quantum mechanics. The limitation of our cognitive capabilities is caused by the fact that the A/D converter is either fast with low resolution and generates numerous errors (flash converter), or very accurate but slow. Thus, in computer-aided experimental systems, thanks to the use of analog-to-digital converters, we either obtain a~massive amount of inaccurate data in a~short time or are satisfied with a~small portion of very precise data. It therefore seems as if measurement accuracy and speed are negatively correlated.



\section*{Conclusions}

The introduction of computer support to experimental research results in the creation of a~``distance'' between the experimenter and the tested object as well as the appearance of completely new artifacts that could not appear in experiments conducted without the use of computers. The introduction of analog-to-digital converters that are part of the experimental system causes the appearance of qualitatively new errors and introduces a~qualitatively new cognitive limitation (speed or accuracy of measurements). Moreover, when using A/D converters, we should be aware that in order to select the appropriate converter for the experimental system we are assembling, we must not only know the principle governing the operation of the measuring device, but also have a~lot of theoretical knowledge about the tested object.



A~similar analysis should also be carried out in relation to the impossibility of archiving all empirical data generated by modern digitally supported experiments. In great research laboratories (e.g. LHC at CERN) it is impossible to archive as little as 1\% of the data generated by detectors, as there are no such massive data repositories that could store this information. It is therefore necessary to delete almost in real time over 99\% of the data representing the processes taking place in the course of the experiment. We should therefore consider to what extent algorithms filtering empirical data deprive us of valuable knowledge about the processes taking place within the framework of the experiment. Are technical difficulties related to archiving empirical data a~sufficient justification for deleting most experimental data? Is this another cognitive limitation of the cognizing entity that has not been sufficiently analyzed? These questions relating to the role of digital elements in the experimental system are still waiting to be developed (discussing them here would excessively expand the scope of this article).



I~am aware that there is a~number of analyzes relating to computer experiments (computer simulations), which, under certain assumptions, could be considered an extension of the concept of the new experimentalism 
%\label{ref:RND046FlgHnlQ}(Bartz-Beielstein, 2005).
\parencite[][]{bartz-beielstein_new_2005}. %
 It seems, however, that the methodological and epistemological aspects of incorporating digital elements into the experimental system are still an important and unrecognized research field of the philosophy of science. Their development would allow to expand the new experiment to such an extent that it could be a~philosophy of experiment of the 21\textsuperscript{st} century and not just a~historical concept dating back to the end of the 20\textsuperscript{th} century.

\enlargethispage{1.5\baselineskip}

\end{artengenv}

